{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Restoring Models\n",
    "Once you have trained your model, why should we save its parameters?\n",
    "1. you can come back to it ewhenever you want.\n",
    "2. using it in anotehr program.\n",
    "3. comparing it to other models, and so on.\n",
    "\n",
    "Moreover, you can prevent from losing your parameters by saving checkpoints at regular intervals during training. So, how? Just create a `Saver` node at the end of the construction phase (after all variable nodes are created). then, in the execution phase, just call its `save()` method whenever you want to save the model, passing it the session and path of the checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Adding an extra bias input feature (x0 = 1) to all training instances\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape # (20640, 8)\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data] # (20640, 9)\n",
    "\n",
    "# Normalize the feature vector: scaled_housing_data_plus_bias\n",
    "scaler =preprocessing.StandardScaler().fit(housing.data)\n",
    "scaled_housing_data = scaler.transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * batch_index)\n",
    "    batch_mark = np.random.choice(m, batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[batch_mark]\n",
    "    y_batch = housing.target.reshape(-1,1)[batch_mark]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") # random_uniform() generating a tensor containing random values\n",
    "y_pred = tf.matmul(X, theta, name=\"prediction\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "# saver = tf.train.Saver({\"weights\": theta})\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0: # checkpoint every 100 epochs\n",
    "            save_path = saver.save(sess, \"./tmp/my_model.ckpt\")\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "            \n",
    "\n",
    "# restoring your model\n",
    "# with tf.Sesson() as sess:\n",
    "    # saver.restore(sess, \"./tmp/my_model_final.ckpt\") \n",
    "\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"./tmp/my_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Graph and Training Curves Using TensorBoard\n",
    "The TensorBoard nicely display interactive visulizations of some taining stats you feed in your webv browser (e.g., learning curves).\n",
    "1. writes the graph definition and some training stats - the training error (MSE).\n",
    "2. need to use a different log directory every time you run your program: include a timestamp in the log directory name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir =  \"./{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") # random_uniform() generating a tensor containing random values\n",
    "y_pred = tf.matmul(X, theta, name=\"prediction\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "# saver = tf.train.Saver({\"weights\": theta})\n",
    "\n",
    "# creates a node in the graph that will evaluate the MSE value \n",
    "# and write it to a TensorBoard-compatible bianry log string called a summary\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "# creates a FileWriter that you will use to write summaries to logfiles in the log directory.\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0: # checkpoint every 100 epochs\n",
    "            save_path = saver.save(sess, \"./tmp/my_model.ckpt\")\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0: # Avoid logging training stats at every single training step\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "# restoring your model\n",
    "# with tf.Sesson() as sess:\n",
    "    # saver.restore(sess, \"./tmp/my_model_final.ckpt\") \n",
    "\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"./tmp/my_model_final.ckpt\")\n",
    "file_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
