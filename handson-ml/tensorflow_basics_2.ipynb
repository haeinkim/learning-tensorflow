{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It starts by fetching the dataset\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Adding an extra bias input feature (x0 = 1) to all training instances\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape # (20640, 8)\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data] # (20640, 9)\n",
    "\n",
    "# Creating two TensorFlow constant nodes, X and y\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\") # (20640, 9)\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name='y') # (20640, 1)\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y) # Review: The Normal Equation\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: The Normal Equation\n",
    "\n",
    "A mathematical equation that gives the result directly\n",
    "\n",
    "$$ \\hat{\\theta} = (\\mathbf{X^T} \\cdot  \\mathbf{X})^{-1} \\cdot \\mathbf{X^T} \\cdot y $$\n",
    "\n",
    "* $\\hat{\\theta}$ is the value of $\\theta$ that minimizes the cost function.\n",
    "* $y$ is the vector of target values containing $y^{(1)}$ to $y^{(m)}$\n",
    "\n",
    "### NumPy vs. TensorFlow: The main benefit\n",
    "TensorFlow will automatically run this on your GPU card\n",
    "\n",
    "\n",
    "# Implementing Gradient Descent\n",
    "\n",
    "Using Batch Gradient Descent instead of the Normal Equation, we will do this:\n",
    "\n",
    "1. by manually computing the gradients\n",
    "2. by using TensorFlow's autodiff feature\n",
    "3. by using Tensorflow's out-of-the-box optimizers\n",
    "\n",
    "**important notes: normalize the input feature vectors first**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Computing the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize the feature vector: scaled_housing_data_plus_bias\n",
    "scaled_housing = StandardScaler().fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 11.1691\n",
      "Epoch 100 MSE = 0.901225\n",
      "Epoch 200 MSE = 0.659847\n",
      "Epoch 300 MSE = 0.622223\n",
      "Epoch 400 MSE = 0.598159\n",
      "Epoch 500 MSE = 0.580397\n",
      "Epoch 600 MSE = 0.567124\n",
      "Epoch 700 MSE = 0.557157\n",
      "Epoch 800 MSE = 0.549641\n",
      "Epoch 900 MSE = 0.543947\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") # random_uniform() generating a tensor containing random values\n",
    "y_pred = tf.matmul(X, theta, name=\"prediction\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using autodiff\n",
    "Simply replcae the `gradients = ...` line in the Gradient Descent code in the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 3.60388\n",
      "Epoch 100 MSE = 0.683695\n",
      "Epoch 200 MSE = 0.613145\n",
      "Epoch 300 MSE = 0.593266\n",
      "Epoch 400 MSE = 0.578705\n",
      "Epoch 500 MSE = 0.567447\n",
      "Epoch 600 MSE = 0.558677\n",
      "Epoch 700 MSE = 0.55181\n",
      "Epoch 800 MSE = 0.546405\n",
      "Epoch 900 MSE = 0.54213\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") # random_uniform() generating a tensor containing random values\n",
    "y_pred = tf.matmul(X, theta, name=\"prediction\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "#gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using an Optimizer\n",
    "You can simply replace the preceding `gradients = ...` and `training_op = ...` lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 5.33706\n",
      "Epoch 100 MSE = 0.698064\n",
      "Epoch 200 MSE = 0.619365\n",
      "Epoch 300 MSE = 0.593583\n",
      "Epoch 400 MSE = 0.57533\n",
      "Epoch 500 MSE = 0.561998\n",
      "Epoch 600 MSE = 0.552235\n",
      "Epoch 700 MSE = 0.545069\n",
      "Epoch 800 MSE = 0.539798\n",
      "Epoch 900 MSE = 0.535911\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") # random_uniform() generating a tensor containing random values\n",
    "y_pred = tf.matmul(X, theta, name=\"prediction\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "# gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "# gradients = tf.gradients(mse, [theta])[0]\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "#training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding Data to the Training Algorithm\n",
    "\n",
    "To implement Mni-batch GRadient Descent, you need a way to replcae X and y at every iteration with the next mini-batch. The placeholder nodes are used to pass the training data to TensorFlow during training. To create a placeholder node,\n",
    "1. call the `placeholder()` function.\n",
    "2. specify the output tensor's data type.\n",
    "3. optionally, you can specify its shape, if you want to enforce it. `None` means \"any size.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  7.  8.]]\n",
      "[[  9.  10.  11.]\n",
      " [ 12.  13.  14.]]\n"
     ]
    }
   ],
   "source": [
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict = {A:[[1,2,3]]})\n",
    "    B_val_2 = B.eval(feed_dict = {A:[[4,5,6], [7,8,9]]})\n",
    "    \n",
    "    print(B_val_1)\n",
    "    print(B_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * batch_index)\n",
    "    batch_mark = np.random.choice(m, batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[batch_mark]\n",
    "    y_batch = housing.target.reshape(-1,1)[batch_mark]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Changing the definition of X and y in the construction phase to make them placeholder nodes\n",
    "\n",
    "# X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\")\n",
    "# y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") # random_uniform() generating a tensor containing random values\n",
    "y_pred = tf.matmul(X, theta, name=\"prediction\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# define the batch size and compute the total number of batches \n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            # fetch the mini-batches one by one, then provide the value of X and y via the feed_dict parameter\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    \n",
    "    best_theta = theta.eval()   \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
