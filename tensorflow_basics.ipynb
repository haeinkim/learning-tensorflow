{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating my first graph and running it in a session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code does not actually perform any computation. In fact, even the variables are not initialized yet. To evaluate this graph, you need to open a TensorFlow _session_ and use it to initialize the variables and evaluate **f**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print (result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having to repeat `sess.run()` all the time is a bit cumbersome. Here is the better way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually running the initializer for every single variable, you can use the `global_variables_initializer()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() # actually initializer all the variables\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside Jupyter, you may prefer to create an `InteractiveSession`. It automatically sets itself as the default session, so you don't need a with block ( but need to close the session manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A TensorFlow program is typically split into two parts: \n",
    "\n",
    "1. Building a computation graph, called the *construction phase*\n",
    "2. Running it, *the execution phase*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The first part: the construction phase\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# The second part: the execution phase\n",
    "with tf.Session() as sess:\n",
    "    init.run() \n",
    "    result = f.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any node you create is automatically added to the default graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can manage multiple independent graphs by creating a new `Graph` and temporarily making it the default graph inside a `with` block, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifecycle of a Node Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variable starts its life when its initializer is run, and it ends when the session is closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5 # y depends on x, which depends on w:  w -> x -> y\n",
    "z = x * 3 \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())\n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate y and z efficiently, w/o evaluating w and x twice as in the previous code, you must ask TensorFlow to evaluate both y and z in just one graph run, as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Linear Regression with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It starts by fetching the dataset\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Adding an extra bias input feature (x0 = 1) to all training instances\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape # (20640, 8)\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data] # (20640, 9)\n",
    "\n",
    "# Creating two TensorFlow constant nodes, X and y\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\") # (20640, 9)\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name='y') # (20640, 1)\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y) # Review: The Normal Equation\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: The Normal Equation\n",
    "\n",
    "A mathematical equation that gives the result directly\n",
    "\n",
    "$$ \\hat{\\theta} = (\\mathbf{X^T} \\cdot  \\mathbf{X})^{-1} \\cdot \\mathbf{X^T} \\cdot y $$\n",
    "\n",
    "* $\\hat{\\theta}$ is the value of $\\theta$ that minimizes the cost function.\n",
    "* $y$ is the vector of target values containing $y^{(1)}$ to $y^{(m)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy vs. TensorFlow: The main benefit\n",
    "TensorFlow will automatically run this on your GPU card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Implementing Gradient Descent\n",
    "\n",
    "Using Batch Gradient Descent instead of the Normal Equation, we will do this:\n",
    "\n",
    "1. by manually computing the gradients\n",
    "2. by using TensorFlow's autodiff feature\n",
    "3. by using Tensorflow's out-of-the-box optimizers\n",
    "\n",
    "**important notes: normalize the input feature vectors first**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Computing the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the feature vector: scaled_housing_data_plus_bias\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler =preprocessing.StandardScaler().fit(housing_data_plus_bias)\n",
    "\n",
    "scaled_housing_data_plus_bias = scaler.transform(housing_data_plus_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 6.09568\n",
      "Epoch 100 MSE = 5.03491\n",
      "Epoch 200 MSE = 4.96098\n",
      "Epoch 300 MSE = 4.92778\n",
      "Epoch 400 MSE = 4.90319\n",
      "Epoch 500 MSE = 4.88385\n",
      "Epoch 600 MSE = 4.86848\n",
      "Epoch 700 MSE = 4.85622\n",
      "Epoch 800 MSE = 4.84638\n",
      "Epoch 900 MSE = 4.83847\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") # random_uniform() generating a tensor containing random values\n",
    "y_pred = tf.matmul(X, theta, name=\"prediction\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply replcae the `gradients = ...` line in the Gradient Descent code in the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 7.07905\n",
      "Epoch 100 MSE = 4.98361\n",
      "Epoch 200 MSE = 4.92228\n",
      "Epoch 300 MSE = 4.89142\n",
      "Epoch 400 MSE = 4.86896\n",
      "Epoch 500 MSE = 4.8524\n",
      "Epoch 600 MSE = 4.84017\n",
      "Epoch 700 MSE = 4.8311\n",
      "Epoch 800 MSE = 4.82436\n",
      "Epoch 900 MSE = 4.81932\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") # random_uniform() generating a tensor containing random values\n",
    "y_pred = tf.matmul(X, theta, name=\"prediction\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "#gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using an Optimizer\n",
    "You can simply replace the preceding `gradients = ...` and `training_op = ...` lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.55318\n",
      "Epoch 100 MSE = 4.88115\n",
      "Epoch 200 MSE = 4.82529\n",
      "Epoch 300 MSE = 4.81718\n",
      "Epoch 400 MSE = 4.8132\n",
      "Epoch 500 MSE = 4.81045\n",
      "Epoch 600 MSE = 4.80848\n",
      "Epoch 700 MSE = 4.80704\n",
      "Epoch 800 MSE = 4.80601\n",
      "Epoch 900 MSE = 4.80526\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") # random_uniform() generating a tensor containing random values\n",
    "y_pred = tf.matmul(X, theta, name=\"prediction\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "# gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "# gradients = tf.gradients(mse, [theta])[0]\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "#training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding Data to the Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Mni-batch GRadient Descent, you need a way to replcae X and y at every iteration with the next mini-batch. The placeholder nodes are used to pass the training data to TensorFlow during training. To create a placeholder node,\n",
    "1. call the `placeholder()` function.\n",
    "2. specify the output tensor's data type.\n",
    "3. optionally, you can specify its shape, if you want to enforce it. `None` means \"any size.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  7.  8.]]\n",
      "[[  9.  10.  11.]\n",
      " [ 12.  13.  14.]]\n"
     ]
    }
   ],
   "source": [
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict = {A:[[1,2,3]]})\n",
    "    B_val_2 = B.eval(feed_dict = {A:[[4,5,6], [7,8,9]]})\n",
    "    \n",
    "    print(B_val_1)\n",
    "    print(B_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * batch_index)\n",
    "    batch_mark = np.random.choice(m, batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[batch_mark]\n",
    "    y_batch = housing.target.reshape(-1,1)[batch_mark]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Changing the definition of X and y in the construction phase to make them placeholder nodes\n",
    "\n",
    "# X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\")\n",
    "# y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") # random_uniform() generating a tensor containing random values\n",
    "y_pred = tf.matmul(X, theta, name=\"prediction\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# define the batch size and compute the total number of batches \n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            # fetch the mini-batches one by one, then provide the value of X and y via the feed_dict parameter\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    \n",
    "    best_theta = theta.eval()   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Restoring Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have trained your model, why should we save its parameters?\n",
    "1. you can come back to it ewhenever you want.\n",
    "2. using it in anotehr program.\n",
    "3. comparing it to other models, and so on.\n",
    "\n",
    "Moreover, you can prevent from losing your parameters by saving checkpoints at regular intervals during training. So, how? Just create a `Saver` node at the end of the construction phase (after all variable nodes are created). then, in the execution phase, just call its `save()` method whenever you want to save the model, passing it the session and path of the checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 MSE =  4.27216\n",
      "Epoch  100 MSE =  4.65465\n",
      "Epoch  200 MSE =  5.03598\n",
      "Epoch  300 MSE =  5.04383\n",
      "Epoch  400 MSE =  4.51255\n",
      "Epoch  500 MSE =  5.10193\n",
      "Epoch  600 MSE =  4.6594\n",
      "Epoch  700 MSE =  4.49823\n",
      "Epoch  800 MSE =  4.84197\n",
      "Epoch  900 MSE =  5.39028\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") # random_uniform() generating a tensor containing random values\n",
    "y_pred = tf.matmul(X, theta, name=\"prediction\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "# saver = tf.train.Saver({\"weights\": theta})\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            _, mse_val = sess.run([training_op, mse], feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 100 == 0: # checkpoint every 100 epochs\n",
    "            save_path = saver.save(sess, \"./tmp/my_model.ckpt\")\n",
    "            print(\"Epoch \", epoch, \"MSE = \", mse_val)\n",
    "\n",
    "# restoring your model\n",
    "# with tf.Sesson() as sess:\n",
    "    # saver.restore(sess, \"./tmp/my_model_final.ckpt\") \n",
    "\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"./tmp/my_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Graph and Training Curves Using TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorBoard nicely display interactive visulizations of some taining stats you feed in your webv browser (e.g., learning curves).\n",
    "1. writes the graph definition and some training stats - the training error (MSE).\n",
    "2. need to use a different log directory every time you run your program: include a timestamp in the log directory name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") # random_uniform() generating a tensor containing random values\n",
    "y_pred = tf.matmul(X, theta, name=\"prediction\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "# saver = tf.train.Saver({\"weights\": theta})\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if epoch % 100 == 0: # checkpoint every 100 epochs\n",
    "                save_path = saver.save(sess, \"./tmp/my_model.ckpt\")\n",
    "                print(\"Epoch \", epoch, \"MSE = \", mse_val)\n",
    "\n",
    "# restoring your model\n",
    "# with tf.Sesson() as sess:\n",
    "    # saver.restore(sess, \"./tmp/my_model_final.ckpt\") \n",
    "\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"./tmp/my_model_final.ckpt\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
